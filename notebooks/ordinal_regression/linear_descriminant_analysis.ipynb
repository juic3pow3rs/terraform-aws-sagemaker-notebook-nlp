{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # Import LDA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing or empty 'review_detail' values per sentiment category:\n",
      "sentiment\n",
      "NEGATIVE    1\n",
      "dtype: int64\n",
      "New dataset size: (15000, 5)\n",
      "sentiment\n",
      "POSITIVE    5000\n",
      "MEDIUM      5000\n",
      "NEGATIVE    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Loading data from a CSV file\n",
    "df = pd.read_csv('balanced_imdb_reviews_20240205-173555.csv')  # Replace with your actual file path\n",
    "\n",
    "# Handling missing or empty strings in 'review_detail'\n",
    "print(\"Missing or empty 'review_detail' values per sentiment category:\")\n",
    "missing_or_empty_count = df[df['review_detail'].isna() | (df['review_detail'] == '')].groupby('sentiment').size()\n",
    "print(missing_or_empty_count)\n",
    "df['review_detail'] = df['review_detail'].fillna('')  # Fill NaN values\n",
    "\n",
    "# Downsampling to 5,000 samples per sentiment category\n",
    "sampled_df = pd.DataFrame()  # Initialize an empty dataframe\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    sampled_df = pd.concat([sampled_df, df[df['sentiment'] == sentiment].sample(n=5000, random_state=42)])\n",
    "\n",
    "# Resetting the index of the sampled dataframe\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"New dataset size: {sampled_df.shape}\")\n",
    "print(sampled_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.85 GiB for an array with shape (4007, 95391) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Training a model using Linear Discriminant Analysis\u001b[39;00m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearDiscriminantAnalysis()  \u001b[38;5;66;03m# Initialize the LDA model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert sparse matrix to array for LDA\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python312\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:628\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    624\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    625\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    626\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith svd solver. Try another solver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    627\u001b[0m         )\n\u001b[1;32m--> 628\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_solve_svd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlsqr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_lstsq(\n\u001b[0;32m    631\u001b[0m         X,\n\u001b[0;32m    632\u001b[0m         y,\n\u001b[0;32m    633\u001b[0m         shrinkage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinkage,\n\u001b[0;32m    634\u001b[0m         covariance_estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_estimator,\n\u001b[0;32m    635\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:506\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis._solve_svd\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    504\u001b[0m Xc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_):\n\u001b[1;32m--> 506\u001b[0m     Xg \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    507\u001b[0m     Xc\u001b[38;5;241m.\u001b[39mappend(Xg \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_[idx, :])\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxbar_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors_ \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.85 GiB for an array with shape (4007, 95391) and data type float64"
     ]
    }
   ],
   "source": [
    "texts = sampled_df['review_detail'].values\n",
    "sentiments = sampled_df['sentiment'].values\n",
    "\n",
    "# Encoding ordinal categories\n",
    "encoder = OrdinalEncoder(categories=[[\"NEGATIVE\", \"MEDIUM\", \"POSITIVE\"]])\n",
    "y = encoder.fit_transform(sentiments.reshape(-1, 1)).flatten()  # Reshape is needed for a single feature\n",
    "\n",
    "# Vectorizing text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check if the test set is too small\n",
    "if X_test.shape[0] < 1:\n",
    "    raise ValueError(\"Test set is too small. Consider reducing the test_size parameter or adding more data.\")\n",
    "\n",
    "# Training a model using Linear Discriminant Analysis\n",
    "model = LinearDiscriminantAnalysis()  # Initialize the LDA model\n",
    "model.fit(X_train.toarray(), y_train)  # Convert sparse matrix to array for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Predicting\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluating with accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generating and printing the classification report\n",
    "report = classification_report(y_test, predictions, target_names=[\"NEGATIVE\", \"MEDIUM\", \"POSITIVE\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: ['NEGATIVE']\n",
      "Accuracy for Custom String: 0\n"
     ]
    }
   ],
   "source": [
    "# Custom string for prediction\n",
    "custom_text = \"not bad. i really liked the movie, altough the ass of the actress was too small\"\n",
    "\n",
    "# Preprocess the custom string\n",
    "custom_text_vectorized = vectorizer.transform([custom_text])\n",
    "\n",
    "# Predict using the trained model\n",
    "custom_prediction = model.predict(custom_text_vectorized)\n",
    "\n",
    "# Decode the predicted sentiment\n",
    "predicted_sentiment = encoder.inverse_transform([custom_prediction])[0]\n",
    "\n",
    "# Optionally, if you have the actual sentiment for the custom string:\n",
    "actual_sentiment = \"POSITIVE\"  # Replace with the actual sentiment for your custom string\n",
    "\n",
    "# Calculate accuracy for the custom prediction (if you have the actual sentiment)\n",
    "if actual_sentiment:\n",
    "    accuracy_custom = 1 if custom_prediction == encoder.transform([[actual_sentiment]])[0] else 0\n",
    "    print(f\"Actual Sentiment: {actual_sentiment}\")\n",
    "    print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "    print(f\"Accuracy for Custom String: {accuracy_custom}\")\n",
    "else:\n",
    "    print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "    print(\"Actual sentiment not provided, cannot calculate accuracy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
